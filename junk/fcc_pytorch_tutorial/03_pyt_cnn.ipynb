{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "CNNs are great for problems that require classification (and sometimes regression) from visual data. CNNs are useful because *they* find the features themselves as opposed to us determining the features to use. We just give the CNN a label and data that is somewhat visual and let it go.\n",
    "The general flow for this notebook will be:\n",
    "1. Explore tools from PyTorch that allow for the import/transformation of different types of visual data\n",
    "2. Importing data image data from a clothing database\n",
    "3. Design a Multi-Class NN model that will be used to classify the different types of clothing in the images\n",
    "4. Examine the results and then create a CNN model and compare the performance to the MCNN\n",
    "5. Save the weights from model with the best results so that it can be used elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by importing everything we will need\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets # contains pre-built datasets that can be used to test models\n",
    "from torchvision.transforms import ToTensor # contains useful functions that can transfrom common images formats to tensors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(f'PyTorch Version: {torch.__version__}, Torchvision Version: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get our training and testing data. Turns out torchvision has many built-in datasets already.\n",
    "# We will be using the FashionMNIST dataset for this classification problem. It's basically the fashion\n",
    "# version of the original MNIST dataset that used number. There are 10 classes, but they're clothes, not numbers.\n",
    "\n",
    "DATA_DIR = \"../../data/\"\n",
    "\n",
    "# Many of the datasets in this module have the same arguments.\n",
    "train_data = datasets.FashionMNIST(\n",
    "    DATA_DIR,\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    DATA_DIR,\n",
    "    train=False,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the training data donwloaded, lets explore\n",
    "print(type(train_data))\n",
    "\n",
    "# Lets get the first image. label combo from the test data\n",
    "image, label = test_data[0]\n",
    "print(f'Label Type: {type(label)}, Label: {label}')\n",
    "print(f'Image Type: {type(image)}, Image Shape: {image.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So it looks like the image is a 3D tensor and the label is just an integer giving the class. The 1 in the first dimension of the image shows that it's just a greyscale image. This implies that the values in the 28x28 2D tensor just represent the intensity of the the pixel. If this were a color image, there would be 3 \"channels\" representing the intensity of RGB respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset object has useful properties that allow you to view different aspects of the dataset.\n",
    "print(f'Training size: {len(train_data)}, Test size: {len(test_data)}, Test/Train Ratio: {len(test_data)/len(train_data):.2f}')\n",
    "print(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets actually look at some of the images/labels in the test dataset\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt_rows = 4\n",
    "plt_cols = 4\n",
    "torch.manual_seed(42)\n",
    "for sub in range(1, plt_rows * plt_cols + 1):\n",
    "    index = torch.randint(0, len(train_data) - 1, size=[1]).item() # Get a random number (in range of the train data)\n",
    "    img, label = train_data[index] # Pull a random test sample and it's label from the training data\n",
    "    fig.add_subplot(plt_rows, plt_cols, sub)\n",
    "    plt.imshow(img.squeeze(), cmap='gray') # Show the image in the subplot. Squeezing out the first dimension, not needed for 2D greyscale\n",
    "    plt.title(train_data.classes[label])\n",
    "    plt.axis(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As seen above, we have quite a few different images corresponding to the different classes in the dataset! The next step is create a Dataloader object for input into the model we plan to create. This seems like an arbitrary step, why do we need to transform the data into some special object as opposed to just using it as it exists in tensor form? The main reason is due to __Batching__. In most real world scenarios, datasets will be huge. So far, we've been doing a form of training called __Batch Gradient Descent__ where *backprop is done on the entire training set for each epoch*. This is computationally possible with small datasets, but with larger datasets, this would be computationally expensive (thus actually expensive in time and cost). The method we'll be adopting is the use of mini-batching; grouping a very small subset of the entire training set to be used for one backprop step. Instead of the loss from the WHOLE dataset (Batch) or each individual sample (Stochastic) being used to change the weights in the model, the *average* loss of the mini-batch is used. This is effective in reducing the computational load and to prevent issues like overfitting. Moving our data to a Dataloader object allows for us to easily change the batch size and to allow for shuffling the data around for each epoch (epoch # = mini-batch #)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders obj for both training and test\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_dloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Now lets inpect the objects. Much of the logic to batch and match labels to input data is done for us in the Dataloader obj\n",
    "print(f'Type: {type(train_dloader)}')\n",
    "train_batch_img, train_batch_labels = next(iter(train_dloader))\n",
    "print(f'Batch Img: {train_batch_img.shape}, Batch Labels: {train_batch_labels.shape}') # So the dloader is basically a list of all the batches and their corressponding labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "With all the data import and preparation done, we can now create a baseline (very simple) model. Will start with a basic 2 layer network, one hidden layer, with a flattening layer as the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMS = train_data[0][0].shape[1]*train_data[0][0].shape[2] # our images are 28*28 and will be flattened out before hitting the NN\n",
    "HIDDEN_UNITS = 10\n",
    "OUTPUT_DIMS = len(train_data.classes)\n",
    "\n",
    "class FashionMNISTModel0(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_shape: int,\n",
    "                 out_shape: int,\n",
    "                 hidden_units: int) -> None:\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=in_shape, out_features=hidden_units, bias=True),\n",
    "            nn.Linear(in_features=hidden_units, out_features=out_shape, bias=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "fmnist_model_0 = FashionMNISTModel0(in_shape=INPUT_DIMS,\n",
    "                               out_shape=OUTPUT_DIMS,\n",
    "                               hidden_units=HIDDEN_UNITS)\n",
    "print(fmnist_model_0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to once again define the optimizer/loss function combo\n",
    "optimizer = torch.optim.SGD(params=fmnist_model_0.parameters(), lr=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss() # 'mean' reduction takes all the loss values from the batch and averages them to get the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's training loop time. Since we're using a slightly different model architecture with mini-batches,\n",
    "# will write out all steps. Mini-batches require a nested loop for each epoch.\n",
    "from timeit import default_timer as timer # boilerplate timer functionality\n",
    "from tqdm.auto import tqdm # text-based progress bar\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Now lets create a quick little function that gives the run time of the loop\n",
    "total_time = lambda start_time, stop_time: stop_time - start_time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "EPOCHS = 2\n",
    "train_size = len(train_dloader.dataset) # number of samples in the train dataset\n",
    "test_size = len(test_dloader.dataset) # number of samples in test dataset\n",
    "num_batches_train = train_size/BATCH_SIZE\n",
    "num_batches_test = test_size/BATCH_SIZE\n",
    "acc = Accuracy(task='multiclass', num_classes=len(test_data.classes))\n",
    "\n",
    "train_time_mnn_start = timer()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f'Epoch: {epoch}\\n-----------')\n",
    "\n",
    "    # Training Steps\n",
    "    train_loss = 0 # need to reset loss every epoch\n",
    "    for batch, (X_train, y_train) in enumerate(train_dloader): # each batch has 32 data/labels, create object -> (batch, (X_train, y_train))\n",
    "        fmnist_model_0.train()\n",
    "        \n",
    "        y_logits = fmnist_model_0(X_train) # Like before, need to get model's predictions (in logits)\n",
    "        loss = loss_fn(y_logits, y_train) # calculate loss for this batch\n",
    "        train_loss += loss # add loss from this batch (mean loss of 32 samples) to total loss for the epoch (sum of all batch loss)\n",
    "\n",
    "        # backprop step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # We want to see some updates within an epoch\n",
    "        print(f'Batches processed: {batch + 1}/{int(num_batches_train)}, Samples processed: {(batch + 1) * BATCH_SIZE}/{train_size}', end='\\r')\n",
    "    \n",
    "    # Now we want to find the AVERAGE loss of all the batches\n",
    "    train_loss /= num_batches_train\n",
    "\n",
    "    # Test Steps; mimick the training steps without backprop.\n",
    "    # Only care about the epoch level values for test, no intermediate\n",
    "    # updates necessary.\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    fmnist_model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dloader:\n",
    "            test_logits = fmnist_model_0(X_test)\n",
    "            test_loss += loss_fn(test_logits, y_test) # Note, no need for batch loss variable since we don't care about per batch backprop when testing\n",
    "            test_acc += acc(test_logits, y_test) # acc expects input tensors denoting labels, need to convert from logits\n",
    "\n",
    "        # Get AVERAGE loss and accuracy off all test batches\n",
    "        test_loss /= num_batches_test \n",
    "        test_acc /= num_batches_test \n",
    "\n",
    "        # print out test loss and test accuracy\n",
    "    print('\\n-----------')\n",
    "    print(f'Mean Test Loss: {test_loss:.4f}')\n",
    "    print(f'Mean Test Accuracy: {test_acc * 100:.2f}%')\n",
    "    print('-----------\\n')\n",
    "train_time_mnn_end = timer()\n",
    "print(f'Total time to train: {total_time(train_time_mnn_start, train_time_mnn_end):.2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seems like the baseline model is doing a decent job, but lets see if we can make it any better by adding non-linearity to the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the train/test loop has gotten more complex, lets turn the steps into functions\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn):\n",
    "\n",
    "    # Training Steps\n",
    "    loss, acc = 0, 0 # need to reset loss every epoch\n",
    "    for batch, (X, y) in enumerate(data_loader): # each batch has 32 data/labels, create object -> (batch, (X, y))\n",
    "        model.train()\n",
    "        y_pred = model(X) # Like before, need to get model's predictions (in logits)\n",
    "        loss = loss_fn(y_pred, y) # calculate loss for this batch\n",
    "        loss += loss # add loss from this batch (mean loss of 32 samples) to total loss for the epoch (sum of all batch loss)\n",
    "        acc += accuracy_fn(y_pred.argmax(dim=1), y)\n",
    "\n",
    "        # backprop step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # We want to see some updates within an epoch\n",
    "        print(f'Batches processed: {batch + 1}/{len(data_loader)}, Samples processed: {(batch + 1) * data_loader.batch_size}/{len(data_loader.dataset)}', end='\\r')\n",
    "    \n",
    "    # Now we want to find the AVERAGE loss and accuracy of all the batches\n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "    print('\\n-----------')\n",
    "    print(f'Mean Train Loss: {loss:.4f}, Mean Train Accuracy: {acc * 100:.2f}%')\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn):\n",
    "\n",
    "    # Test Steps\n",
    "    model.eval()\n",
    "    loss, acc = 0, 0 # need to reset loss every epoch\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader: # each batch has 32 data/labels, create object -> (batch, (X_train, y_train))\n",
    "            y_pred = model(X) # Like before, need to get model's predictions (in logits)\n",
    "            loss = loss_fn(y_pred, y) # calculate loss for this batch\n",
    "            loss += loss # add loss from this batch (mean loss of 32 samples) to total loss for the epoch (sum of all batch loss)\n",
    "            acc += accuracy_fn(y_pred.argmax(dim=1), y)\n",
    "\n",
    "        # Now we want to find the AVERAGE loss and accuracy of all the batches\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    print(f'Mean Test Loss: {loss:.4f}, Mean Test Accuracy: {acc * 100:.2f}%')\n",
    "    print('-----------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try out new functions out in an identical training loop as before\n",
    "# Now it's training loop time. Since we're using a slightly different model architecture with mini-batches,\n",
    "# will write out all steps. Mini-batches require a nested loop for each epoch.\n",
    "\n",
    "from helper_functions import model_accuracy\n",
    "torch.manual_seed(42)\n",
    "EPOCHS = 2\n",
    "train_time_mnn_start = timer()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f'Epoch: {epoch}\\n-----------')\n",
    "    train_step(\n",
    "        fmnist_model_0,\n",
    "        train_dloader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        model_accuracy\n",
    "    )\n",
    "    test_step(\n",
    "        fmnist_model_0,\n",
    "        test_dloader,\n",
    "        loss_fn,\n",
    "        model_accuracy\n",
    "    )\n",
    "train_time_mnn_end = timer()\n",
    "print(f'Total time to train: {total_time(train_time_mnn_start, train_time_mnn_end):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this simplified training/testing loop code, lets define a new model with non-linearities and try to test it again\n",
    "INPUT_DIMS = train_data[0][0].shape[1]*train_data[0][0].shape[2] # our images are 28*28 and will be flattened out before hitting the NN\n",
    "HIDDEN_UNITS = 10\n",
    "OUTPUT_DIMS = len(train_data.classes)\n",
    "\n",
    "class FashionMNISTModel1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_shape: int,\n",
    "                 out_shape: int,\n",
    "                 hidden_units: int) -> None:\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=in_shape, out_features=hidden_units, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=out_shape, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "fmnist_model_1 = FashionMNISTModel1(in_shape=INPUT_DIMS,\n",
    "                               out_shape=OUTPUT_DIMS,\n",
    "                               hidden_units=HIDDEN_UNITS)\n",
    "print(fmnist_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "optimizer_v1 = torch.optim.SGD(params=fmnist_model_1.parameters(), lr=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss() # 'mean' reduction takes all the loss values from the batch and averages them to get the loss\n",
    "\n",
    "EPOCHS = 2\n",
    "train_time_mnn_start = timer()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f'Epoch: {epoch}\\n-----------')\n",
    "    train_step(\n",
    "        fmnist_model_1,\n",
    "        train_dloader,\n",
    "        loss_fn,\n",
    "        optimizer_v1,\n",
    "        model_accuracy\n",
    "    )\n",
    "    test_step(\n",
    "        fmnist_model_1,\n",
    "        test_dloader,\n",
    "        loss_fn,\n",
    "        model_accuracy\n",
    "    )\n",
    "train_time_mnn_end = timer()\n",
    "print(f'Total time to train: {total_time(train_time_mnn_start, train_time_mnn_end):.2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "Now with the multi-class NN complete, lets update the model to use a CNN! We'll be using the model architecture called \"TinyVGG\" from the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) website. Let's now create some test data to explore what the different types of layers used in the model do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define some constants and then look at nn.Conv2d\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 3\n",
    "SAMPLE_HEIGHT = 64\n",
    "SAMPLE_WIDTH = 64\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "train_sample = torch.randn(size=(BATCH_SIZE, CHANNELS, SAMPLE_HEIGHT, SAMPLE_WIDTH))\n",
    "print(train_sample.dtype)\n",
    "print(f'First channel of the first training sample:\\n {train_sample[0][0]}')\n",
    "print(f'Shape of the first training sample: {train_sample[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a Conv2d layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, # num channels in the input data\n",
    "                       out_channels=10, # num channels (filters) in the output data\n",
    "                       kernel_size=3, # size of sliding window that does the convolution (compression)\n",
    "                       stride=1, # the \"step\" size of the kernel\n",
    "                       padding=0) # elements added to the input tensor boundaries to manipulate the output size\n",
    "\n",
    "conv_output = conv_layer(train_sample[0]) # send the first training sample from the batch through the conv2d layer\n",
    "print(f'Shape of sample before conv: {train_sample[0].shape}')\n",
    "print(f'Shape of sample after conv: {conv_output.shape}') \n",
    "print(f'Sample after convolution:\\n {conv_output}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note how the size of the images is compressed after the convolution. This is expected because we want to only get the most relevant information from the training sample, which could initially be a large tensor!. Also, we increased the channel size by quite a bit. Output channels can also be called **filters**. Under the hood, there is a **kernel** for each channel in the previous layer *for each neuron in the conv layer*. For example, there are 3 kernels per neuron in the first conv layer since the training sample has 3 channels. Since we defined the conv2d input layer to have 10 output channels, this corresponds to the *number of neurons in the layer*. With CNNs, the *learned weights by the model are stored in the kernels!* The kernels sweep the input image from the previous layer (called an **activation map**) and do element wise multiplication with the values in the input activation map of the corresponding channel and the learned weights in the kernel (there is a weight per element in each kernel). Note that for each neuron, the results of the convolution of each kernel with the corressponding activation map of the previous layer are *summed*, elementwise, along with the bias and then are passed along as an activation map for the next layer. The CNN Explainer website has a beautiful visual for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets explore the other new layer, nn.MaxPool2d. Any time \"pool\" is in a name, it means compression.\n",
    "# So the MaxPool2d layer is similar to the conv2d layer in that there is a kernel that sweeps along the\n",
    "# input image, but instead of the convolution operation being done, the kernel selects the maximum value\n",
    "# of the values that are \"covered\" by the kernel at a given step. So for a 2x2 kernel, the maxiumum value\n",
    "# from a 2x2 grid is selected and added to the activation map. Note that MaxPool layers are not fully connected,\n",
    "# they are one-to-one. These typically come after an activation function has been applied to the activation maps\n",
    "# of the neurons in the previous conv layer. A 2x2 kernel size for MaxPool2d will cut the image size in half! (compression)\n",
    "pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "pool_output = pool(train_sample[0])\n",
    "print(f'Shape of sample before MaxPool: {train_sample[0].shape}')\n",
    "print(f'Shape of sample after MaxPool: {pool_output.shape}') \n",
    "print(f'Sample after Pooling:\\n {pool_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets build the model! We'll be using the TinyVGG architecture while changing some params to fit\n",
    "# the FashionMNIST data.\n",
    "KERNEL_SIZE = 3\n",
    "PADDING = 1\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "class FashionMNISTmodel_cnn(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_units, output_shape):\n",
    "        super().__init__()\n",
    "        # Lets define the first \"block\" from TinyVGG (Conv2d -> ReLU -> Conv2d -> ReLU -> MaxPool)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=hidden_units, kernel_size=KERNEL_SIZE, stride=1, padding=PADDING), # padding keeps sample img size the same thru layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=KERNEL_SIZE, stride=1, padding=PADDING),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # Lets define the second \"block\" from TinyVGG (Conv2d -> ReLU() -> Conv2d -> ReLU -> MaxPool)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=KERNEL_SIZE, stride=1, padding=PADDING), # padding keeps sample img size the same thru layer\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=KERNEL_SIZE, stride=1, padding=PADDING),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # Finally, lets define the last block, which flattens the activation maps after the final pooling layer and then does the classification\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=7*7*hidden_units, out_features=output_shape, bias=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block3(self.block2(self.block1(x)))\n",
    "    \n",
    "fmnist_model_2_cnn = FashionMNISTmodel_cnn(in_channels=1, hidden_units=10, output_shape=len(train_data.classes))\n",
    "fmnist_model_2_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create an optimizer/loss function and run the train/test loop\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_v2 = torch.optim.SGD(params=fmnist_model_2_cnn.parameters(), lr=0.1)\n",
    "\n",
    "EPOCHS = 5\n",
    "train_time_mnn_start = timer()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f'Epoch: {epoch}\\n-----------')\n",
    "    train_step(\n",
    "        fmnist_model_2_cnn,\n",
    "        train_dloader,\n",
    "        loss_fn,\n",
    "        optimizer_v2,\n",
    "        model_accuracy\n",
    "    )\n",
    "    test_step(\n",
    "        fmnist_model_2_cnn,\n",
    "        test_dloader,\n",
    "        loss_fn,\n",
    "        model_accuracy\n",
    "    )\n",
    "train_time_mnn_end = timer()\n",
    "print(f'Total time to train: {total_time(train_time_mnn_start, train_time_mnn_end):.2f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results above with the given seed are encouraging! The performance of the CNN is an improvement compared to the previous two models. Now lets explore a more useful way of determining the performance of the model by using a confusion matrix. This will give us a better idea of *how* the model is underperforming by showing us the classes that the model mis-classifies the most and which classes are \"confused\" the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets create a function that does inference based on the trained model\n",
    "def make_predictions(model: nn.Module, samples: list):\n",
    "    \"\"\"\n",
    "    Given a list of samples, returns a list of the probabilities for each class for each sample\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        preds_list = [torch.softmax(model(x), dim=1) for x in tqdm(samples, desc=\"Making predictions...\")] # apply softmax to logits of each sample in sample list\n",
    "        return torch.cat(preds_list) # return as tensor instead of list\n",
    "\n",
    "\n",
    "# Pick n random samples/labels from the test data\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "import random\n",
    "for sample, label in random.sample(list(test_data), k=10): # random.sample samples k elements from the given population without replacement; returns list of samples.\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "\n",
    "print(f'List size: {len(test_labels)}')\n",
    "print(f'First sample shape from list: {test_samples[0].shape}')\n",
    "print(f'First label from list: {test_labels[0]}')\n",
    "\n",
    "# Now lets make the predictions and inspect\n",
    "probs = make_predictions(fmnist_model_2_cnn, [x.unsqueeze(dim=0) for x in test_samples]) # need to add a dimension to the data to get it through the model.\n",
    "print(f'probs shape: {probs.shape}')\n",
    "print(f'probs length: {len(probs)}')\n",
    "print(f'probs for first sample:\\n {probs[0]}')\n",
    "\n",
    "# Now lets collapse the tensors of probabilities to the label\n",
    "preds = torch.argmax(probs.squeeze(), dim=1) # tensor is 10x1x10, squeeze out the unity dim. and apply argmax in the x direction for each sample (dim 1)\n",
    "print(f'preds length: {len(preds)}')\n",
    "print(f'label prediction for first sample: {preds[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, preds # These two are very similar, the 7th sample has the incorrect prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a way to get predictions from the model using the test data, we can now create a confusion matrix.\n",
    "# Pytorch already has a confusion matrix implementation that we can use and mlxtend is a package that plots the matrix\n",
    "# in a useful way for us.\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Lets get logits from the FULL test dataset now and then send to argmax to get label predictions\n",
    "preds = torch.argmax(make_predictions(fmnist_model_2_cnn, [X for X, _ in test_dloader]), dim=1) # get all the sample tensors from test_dloader and get the predictions.\n",
    "\n",
    "# With the predictions on the entire test dataset, can now create and plot the confusion matrix\n",
    "confmat = ConfusionMatrix(task='multiclass', num_classes=len(test_data.classes))\n",
    "confmat_tensor = confmat(preds=preds, target=test_data.targets)\n",
    "\n",
    "# Plot the confustion matrix with class labels\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    confmat_tensor.numpy(), class_names=test_data.classes\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the confusion matrix, most predictions are accurate, but the model seems to get confused between the \"t-shirt/top\" and \"shirt\" classes and between the \"shirt\" and \"coat\" classes the most!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iq_ml_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
