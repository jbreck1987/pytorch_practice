{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification\n",
    "Now that we've explored a binary classification problem with linear and non-linear architectures, we now want to shift to a multi-class problem where there are more than two options that the model needs to be able to classify.\n",
    "\n",
    "The multi-class data will be artificial data from the scikit-learn `make_blobs()` function. The general flow is as follows:\n",
    "1. Make the artificial data and convert to tensors\n",
    "2. Visualize the data\n",
    "3. Define the model architecture\n",
    "4. Train the model\n",
    "5. Adjust hyperparameters as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets explore the make_blobs() function. According to the documentation,\n",
    "# make_blobs is designed for creating artificial multiclass data by creating isotropic, Gaussian clusters\n",
    "# of points. The data is quite literally \"blobs\" of points around a \"center\" in R^n space. The classes could be based \n",
    "# on the number of centers in a feature set.\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "n_points = 50\n",
    "X_blob, y_blob = make_blobs([n_points, n_points], # array length = num blobs, n_points = points per blob\n",
    "                            n_features=3,\n",
    "                            centers=None, random_state=42) # returns coordinates of points (X) and its blob membership\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('Two blobs with 3 features (x, y, z)')\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_blob[:,0], X_blob[:,1], X_blob[:, 2], c=y_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets standardize this a little for the actual model. Will define the const values that will be used when creating\n",
    "# the architecture (allow things to be updated once). This could probably eventually be refactored into a dataclass.\n",
    "\n",
    "NUM_CLASSES = 4 # This is self explanatory, the number of blobs per training data instance\n",
    "CLUSTER_POINTS = 100 # This is the number of points that are in each blob\n",
    "NUM_FEATURES = 2 # This refers to the dimension of the data. In this case, the dimension of the the points in the blobs (the above example is 3D)\n",
    "CLUSTER_STD_DEV = 1.0 # This changes the spread in each blob (makes classification more difficult!)\n",
    "RANDOM_SEED = 42\n",
    "TRAIN_TEST_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create our training data and move to tensors\n",
    "X_blob, y_blob = make_blobs([CLUSTER_POINTS for centers in range(NUM_CLASSES)],\n",
    "                            n_features=NUM_FEATURES,\n",
    "                            centers=None,\n",
    "                            random_state=RANDOM_SEED,\n",
    "                            cluster_std=CLUSTER_STD_DEV)\n",
    "\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_blob,\n",
    "    y_blob,\n",
    "    test_size=TRAIN_TEST_RATIO, # Ratio of test data to use from full dataset; Training is the complement\n",
    "    random_state=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets inspect our dataset to make sure it looks as expected\n",
    "print(X_train[0:9]) # expect coordinates from R^(NUM_FEATURES)\n",
    "print(y_train[0:14]) # expect values from 0-(NUM_CLASSES -1)\n",
    "print(f'X ratio: {len(X_test)/len(X_train)}, y ratio: {len(y_test)/len(y_train)}') # should be ~TRAIN_TEST_RATIO\n",
    "for obj in [X_train, X_test, y_train, y_test]: # expecting all to be torch.float\n",
    "    print(obj.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the dataset properties look good, lets visualize it!\n",
    "fig = plt.figure()\n",
    "base_title = f'{NUM_CLASSES} blobs with {NUM_FEATURES} features'\n",
    "if NUM_FEATURES >= 3:\n",
    "    plt.title(base_title + ' (first 3 dims.)')\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], X_train[:, 2], c=y_train)\n",
    "elif NUM_FEATURES == 2:\n",
    "    plt.title(base_title)\n",
    "    plt.scatter(X_train[:,0], X_train[:,1], c=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets (again) create a linear model first to see how well it performs on the dataset.\n",
    "# Unlike before, we want the architecture to be somewhat modular; we'll leave the number of hidden\n",
    "# layers fixed, but the number of hidden units and input dims will be modular.\n",
    "\n",
    "class LinearBlobModel(nn.Module):\n",
    "    def __init__(self, num_in_features: int, num_hidden_units: int, num_out_features: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=num_in_features, out_features=num_hidden_units), # Note that these are LINEAR layers (activation f'n is linear...)\n",
    "            nn.Linear(in_features=num_hidden_units, out_features=num_hidden_units),\n",
    "            nn.Linear(in_features=num_hidden_units, out_features=num_out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "lin_model_0 = LinearBlobModel(NUM_FEATURES, 8, NUM_CLASSES) \n",
    "lin_model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the model is created, lets see what the output looks like\n",
    "with torch.inference_mode():\n",
    "    lin_model_0.eval()\n",
    "    untrained_logits = lin_model_0(X_train)\n",
    "untrained_logits[0:4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model spits out an array of logit values for each training point. Each positiion in the array corresponds to a class. The ouput logit value for each class determines the \"confidence\" the model has that the training sample is a member point for that class. We can get these logits to probabilities by using the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_preds = torch.softmax(untrained_logits, dim=1)\n",
    "print(untrained_preds[0:4])\n",
    "print(f'Predicted class for training sample 0: {torch.argmax(untrained_preds[0][0:4])}')\n",
    "print(f'Actual class for training sample 0: {y_train[0]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the values are between 0 and 1 and the sum of the output values for a given training sample sum to 1. In the printed example, the class at index 2 has the highest probability, so the model is saying it thinks that training sample 0 is a member of class 2. The model gets it wrong, as expected, since it's untrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the linear blob model created, lets again go ahead and define the optimizer and loss functions.\n",
    "# Since we're dealing with multiclass classification now, we need to use a loss function that is multivariable.\n",
    "# CrossEntropyLoss, it's your time! It's just the multi-class version of BCE\n",
    "optimizer = torch.optim.SGD(lr=0.2, params=lin_model_0.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # docs say this loss fn expects LOGITS, not probabilities like BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iq_ml_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
