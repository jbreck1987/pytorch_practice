{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like in math, tensors can be different rank\n",
    "\n",
    "# Rank 0 Tensor\n",
    "scalar = torch.tensor(9)\n",
    "\n",
    "# Rank 1 Tensor\n",
    "vector = torch.tensor([1,2])\n",
    "\n",
    "# Rank 2 Tensor\n",
    "matrix = torch.tensor(np.random.rand(2,2))\n",
    "\n",
    "# Rank 3 Tensor\n",
    "tensor = torch.tensor(np.random.rand(2,2,2))\n",
    "\n",
    "print(scalar.ndim)\n",
    "print(vector.ndim)\n",
    "print(matrix.ndim)\n",
    "print(tensor.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like Pytorch indexes tensors using math convention\n",
    "# M_ij, where i is the row and j is the column (with 0 indexing)\n",
    "\n",
    "print(matrix)\n",
    "print(matrix[0])\n",
    "print(matrix[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 3 Tensors are indexed like numpy (plane, row, column)\n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "print(tensor[0]) # Display first plane\n",
    "print(tensor[0][0]) # Display the first row in the first plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random Tensors\n",
    "# Very useful for initializing weights in ML models\n",
    "\n",
    "rand_tensor = torch.rand(2,3,3) # Better API than numpy :P\n",
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like with Numpy, can create zeros and ones Tensors\n",
    "zero_tensor = torch.zeros(3,5,5)\n",
    "print(zero_tensor)\n",
    "\n",
    "ones_tensor = torch.ones(size=(4,2,2), dtype=torch.float64) # Can be explicit about the kwarg for size and dtype\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can create Tensors with ranges of values (just like arange in Numpy...)\n",
    "\n",
    "range_tensor = torch.arange(0, 20, 2) # Can only be used for rank 1 tensors\n",
    "print(range_tensor.shape)\n",
    "print(range_tensor.ndim)\n",
    "print(range_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can create Tensors with same dimension (and other attributes, like datatype and device!) as another existing tensor\n",
    "# to help decrease issues with size mismatches with certain operations\n",
    "\n",
    "alike_tensor = torch.rand_like(input=ones_tensor)\n",
    "print(alike_tensor)\n",
    "print(alike_tensor.ndim) # Same rank as the input tensor\n",
    "print(alike_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor datatype exploration\n",
    "\n",
    "# dtype and device are obvious; requires_grad specifies if the tensor will be differentiated\n",
    "# with Autograd. Default device is cpu\n",
    "# Default is float32 and int64\n",
    "int_tensor = torch.ones(4,4, dtype=torch.int16, device=\"mps\") #mps is the M1 GPU!\n",
    "print(int_tensor.device, int_tensor.dtype, int_tensor.requires_grad) # Important attributes!\n",
    "\n",
    "# Output will only display dtype if using non-defaults\n",
    "print(int_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same ops as Numpy, including broacasting\n",
    "big_tensor_1 = torch.rand(10000, 10000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_2 = torch.rand(10000, 10000, dtype=torch.float32, device=\"mps\")\n",
    "\n",
    "# Element wise multiplication (* used as alias for element-wise multiplication)\n",
    "big_tensor_3 = torch.mul(big_tensor_1, big_tensor_2)\n",
    "print(big_tensor_3.shape)\n",
    "\n",
    "# Matrix multiplication (changing shape to show resultant matrix from matmul op)\n",
    "big_tensor_2 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_3 = torch.matmul(big_tensor_1, big_tensor_2)\n",
    "print(big_tensor_3.shape)\n",
    "\n",
    "# Alias for matmul() is @, just like Numpy\n",
    "big_tensor_3 = big_tensor_1@big_tensor_2\n",
    "print(big_tensor_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like Numpy, .T takes the transpose of a Tensor\n",
    "big_tensor_1 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_2 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_3 = torch.matmul(big_tensor_1, big_tensor_2) # Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_tensor_1 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_2 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_3 = torch.matmul(big_tensor_1, big_tensor_2.T) # Happy\n",
    "print(big_tensor_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has built-in aggregation functions (min, max, sum, mean, etc.)\n",
    "print(torch.max(big_tensor_3))\n",
    "print(torch.min(big_tensor_3))\n",
    "print(torch.mean(big_tensor_3))\n",
    "print(torch.sum(big_tensor_3))\n",
    "\n",
    "# Tensors also have these methods built-in\n",
    "print(big_tensor_3.max())\n",
    "print(big_tensor_3.min())\n",
    "print(big_tensor_3.mean())\n",
    "print(big_tensor_3.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like Numpy, also have argmax and argmin\n",
    "vector = torch.rand(10)\n",
    "matrix = torch.rand(size=(3,3))\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "print(matrix)\n",
    "print(matrix.shape)\n",
    "print(vector.argmax()) # Gives index with largest value, *returned as a Tensor!*\n",
    "print(vector.argmin()) # Gives index with largest value, *returned as a Tensor!*\n",
    "print(matrix.argmax()) # Gives index with largest value, *returned as a Tensor!* Note that the index is singular here, each location in the matrix is indexed sequentially.\n",
    "print(matrix.argmin()) # Gives index with largest value, *returned as a Tensor!* Note that the index is singular here, each location in the matrix is indexed sequentially.\n",
    "\n",
    "# When setting the dimension, it gives the index with the largest value along the given axis for each row or column\n",
    "# NOTE: The indexing notation seems to be flipped here :/...dim 0 -> cols, dim 1 -> rows\n",
    "print(matrix.argmax(dim=0)) # returns a tensor with the index of the largest value for each column\n",
    "print(matrix.argmin(dim=1)) # returns a tensor with the index of the smallest value for each row"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "\n",
    "Just like with Numpy, Tensors can be reshaped, stacked, and squeezed, etc. The number one issue in ML code is mismatching tensor shapes for tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic tensor\n",
    "vec = torch.arange(0., 20, 2)\n",
    "print(vec.shape)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets reshape this rank 0 tensor into a rank 2 tensor\n",
    "mat = vec.reshape(2,5) # Total number of elements in reshaped array must match the input\n",
    "print(mat)\n",
    "\n",
    "# Lets reshape into rank 3\n",
    "mat = vec.reshape(3, 3, 5)\n",
    "print(mat) # Error! Cannot reshape into rank higher than 2 because there are only two prime factors in a 10 element tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A view of a Tensor is like a reference in c++; the view of a tensor refers to the same memory location as input tensor.\n",
    "# Useful when dealing with large tensors that need to be reshaped\n",
    "\n",
    "v = vec.view(vec.shape)\n",
    "print(v)\n",
    "print(vec)\n",
    "\n",
    "# Views are powerful when you need a reshaped \"presentation\" of an existing tensor.\n",
    "# If you have an existing tensor but ONLY need the shape to change, but not the values,\n",
    "# a view is useful because it doesn't duplicate data (this is hinting that the shapes of tensors and\n",
    "# the actual data contained with them are not linked...hmmm.)\n",
    "v = vec.view(2,5)\n",
    "print(vec)\n",
    "print(v)\n",
    "vec[0] = 9 # Should change the value in the first index from 0 to 9 in vec AND the reshape v\n",
    "print(v)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The permute method returns a *view* of an existing tensor with the dimensions rearranged\n",
    "# however you want them (very useful for, say, images that come in height, width, channels but\n",
    "# tensors naturally like the channel (aka \"planes\") to be in first dim)\n",
    "# NOTE: permute returns a view, so the underlying data is shared with the input tensor\n",
    "\n",
    "img = torch.rand(224, 240,3)\n",
    "print(img.shape)\n",
    "\n",
    "img_p = img.permute(2,0,1) # Moves channel to dim 0, height to dim 1, width to dim 2\n",
    "print(img_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iq_ml_cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bbff50bfc5161648b6e194634e05a5fcf9698595f74ac8274cfa4850e6a92e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
