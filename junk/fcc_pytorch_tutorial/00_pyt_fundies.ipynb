{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like in math, tensors can be different rank\n",
    "\n",
    "# Rank 0 Tensor\n",
    "scalar = torch.tensor(9)\n",
    "\n",
    "# Rank 1 Tensor\n",
    "vector = torch.tensor([1,2])\n",
    "\n",
    "# Rank 2 Tensor\n",
    "matrix = torch.tensor(np.random.rand(2,2))\n",
    "\n",
    "# Rank 3 Tensor\n",
    "tensor = torch.tensor(np.random.rand(2,2,2))\n",
    "\n",
    "print(scalar.ndim)\n",
    "print(vector.ndim)\n",
    "print(matrix.ndim)\n",
    "print(tensor.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like Pytorch indexes tensors using math convention\n",
    "# M_ij, where i is the row and j is the column (with 0 indexing)\n",
    "\n",
    "print(matrix)\n",
    "print(matrix[0])\n",
    "print(matrix[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 3 Tensors are indexed like numpy (plane, row, column)\n",
    "print(tensor.shape)\n",
    "print(tensor)\n",
    "print(tensor[0]) # Display first plane\n",
    "print(tensor[0][0]) # Display the first row in the first plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random Tensors\n",
    "# Very useful for initializing weights in ML models\n",
    "\n",
    "rand_tensor = torch.rand(2,3,3) # Better API than numpy :P\n",
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like with Numpy, can create zeros and ones Tensors\n",
    "zero_tensor = torch.zeros(3,5,5)\n",
    "print(zero_tensor)\n",
    "\n",
    "ones_tensor = torch.ones(size=(4,2,2), dtype=torch.float64) # Can be explicit about the kwarg for size and dtype\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can create Tensors with ranges of values (just like arange in Numpy...)\n",
    "\n",
    "range_tensor = torch.arange(0, 20, 2) # Can only be used for rank 1 tensors\n",
    "print(range_tensor.shape)\n",
    "print(range_tensor.ndim)\n",
    "print(range_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can create Tensors with same dimension (and other attributes, like datatype and device!) as another existing tensor\n",
    "# to help decrease issues with size mismatches with certain operations\n",
    "\n",
    "alike_tensor = torch.rand_like(input=ones_tensor)\n",
    "print(alike_tensor)\n",
    "print(alike_tensor.ndim) # Same rank as the input tensor\n",
    "print(alike_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor datatype exploration\n",
    "\n",
    "# dtype and device are obvious; requires_grad specifies if the tensor will be differentiated\n",
    "# with Autograd. Default device is cpu\n",
    "# Default is float32 and int64\n",
    "int_tensor = torch.ones(4,4, dtype=torch.int16, device=\"mps\") #mps is the M1 GPU!\n",
    "print(int_tensor.device, int_tensor.dtype, int_tensor.requires_grad) # Important attributes!\n",
    "\n",
    "# Output will only display dtype if using non-defaults\n",
    "print(int_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same ops as Numpy, including broacasting\n",
    "big_tensor_1 = torch.rand(10000, 10000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_2 = torch.rand(10000, 10000, dtype=torch.float32, device=\"mps\")\n",
    "\n",
    "# Element wise multiplication (* used as alias for element-wise multiplication)\n",
    "big_tensor_3 = torch.mul(big_tensor_1, big_tensor_2)\n",
    "print(big_tensor_3.shape)\n",
    "\n",
    "# Matrix multiplication (changing shape to show resultant matrix from matmul op)\n",
    "big_tensor_2 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_3 = torch.matmul(big_tensor_1, big_tensor_2)\n",
    "print(big_tensor_3.shape)\n",
    "\n",
    "# Alias for matmul() is @, just like Numpy\n",
    "big_tensor_3 = big_tensor_1@big_tensor_2\n",
    "print(big_tensor_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like Numpy, .T takes the transpose of a Tensor\n",
    "big_tensor_1 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_2 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_3 = torch.matmul(big_tensor_1, big_tensor_2) # Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_tensor_1 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_2 = torch.rand(10000, 8000, dtype=torch.float32, device=\"mps\")\n",
    "big_tensor_3 = torch.matmul(big_tensor_1, big_tensor_2.T) # Happy\n",
    "print(big_tensor_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has built-in aggregation functions (min, max, sum, mean, etc.)\n",
    "print(torch.max(big_tensor_3))\n",
    "print(torch.min(big_tensor_3))\n",
    "print(torch.mean(big_tensor_3))\n",
    "print(torch.sum(big_tensor_3))\n",
    "\n",
    "# Tensors also have these methods built-in\n",
    "print(big_tensor_3.max())\n",
    "print(big_tensor_3.min())\n",
    "print(big_tensor_3.mean())\n",
    "print(big_tensor_3.sum())\n",
    "\n",
    "# Like Numpy, also have argmax and argmin\n",
    "print(big_tensor_3.shape)\n",
    "print(big_tensor_3.argmax(dim=0)) # Gives index in the each row that contains the largest value as a tensor\n",
    "print(big_tensor_3.argmax(dim=0).shape)\n",
    "print(big_tensor_3.argmax(dim=1)) # Gives index in the each column that contains the largest value as a tensor\n",
    "print(big_tensor_3.argmax(dim=1).shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iq_ml_cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bbff50bfc5161648b6e194634e05a5fcf9698595f74ac8274cfa4850e6a92e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
