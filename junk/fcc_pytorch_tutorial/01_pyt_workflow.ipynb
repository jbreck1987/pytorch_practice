{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # contains all the neural network building blocks\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this notebook is to explore an example PyTorch workflow\n",
    "1. Data Preparation\n",
    "2. Building or using an existing model\n",
    "3. Fitting the model to the data (training)\n",
    "4. Making precitions and evaluating a model (inference)\n",
    "5. Saving and loading a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data (prep and load)\n",
    "The original data can come in many forms. The goal is to reformat the data numerically such that it can be represented by tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets form the input data by using the output of a 1st degree polynomial with known slope (weight) and intercept (bias)\n",
    "def lin_reg(X: torch.Tensor, W: torch.Tensor, b: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Returns the result of evaluating a first order polynomial given the weight and bias values and an input\n",
    "    vector\n",
    "\n",
    "    Inputs\n",
    "        x: independent variable vector\n",
    "        W: the weight value (slope)\n",
    "        b: the bias value (intercept)\n",
    "\n",
    "    Outputs\n",
    "        tensor of size x.shape, W.shape\n",
    "    \"\"\"\n",
    "\n",
    "    return W * X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of this cell will be used as input to train (and test, using different input range) the model\n",
    "\n",
    "# Define range\n",
    "start = 0\n",
    "stop = 1\n",
    "step = 0.02\n",
    "\n",
    "# Define bias and weight values\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "X = torch.arange(start, stop, step).unsqueeze(dim=1) # Creating a column vector\n",
    "y = lin_reg(X, weight, bias)\n",
    "print(X[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to create a model that can learn the mathematical function that relates the feature variables (X) to the data (out)\n",
    "# First we, need to divide the data (feature AND output) into 3 distinct sets: Test, Validation, Training\n",
    "# For this example, only train and test sets will be used\n",
    "\n",
    "# Train set is used to train the model (typically 80% of the data)\n",
    "train_stop = int(0.8 * len(X))\n",
    "train_X = X[:train_stop]\n",
    "train_y = y[:train_stop]\n",
    "\n",
    "# Test set is used to test accuracy of model on unseen data\n",
    "test_X = X[train_stop:]\n",
    "test_y = y[train_stop:]\n",
    "\n",
    "print(f'Test X Len: {len(test_X)}\\nTrain X: {len(train_X)}\\nTest y: {len(test_y)}\\nTrain y: {len(train_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to create a function to visualize the data along with the models predictions (no predictions yet...)\n",
    "def plot_data(feature_train: torch.Tensor, feature_test: torch.Tensor, label_train: torch.Tensor, label_test: torch.Tensor, preds: torch.Tensor=None):\n",
    "    \"\"\"\n",
    "    Creates plot depicting the output vs feature for training, test, and prediction data\n",
    "\n",
    "    Inputs\n",
    "        feature_train: Tensor with the feature training data\n",
    "        feature_test: Tensor with the feature test data\n",
    "        output_train: Tensor with the output training data\n",
    "        output_test: Tensor with the output test data\n",
    "        preds: Tensor with the predictions of model given the feature_test data as input\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Labels')\n",
    "    # plot the feature vs output for training data\n",
    "    plt.scatter(feature_train, label_train, c='b', s=3, label='Training Data')\n",
    "    # plot the feature vs output for test data\n",
    "    plt.scatter(feature_test, label_test, c='g', s=3, label='Test Data')\n",
    "    # plot the feature vs prediction (feature_test as input)\n",
    "    if preds is not None:\n",
    "        plt.scatter(feature_test, preds, c='r', s=3, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As expected the function between the feature data and the output data is linear, based on the defined function\n",
    "plot_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to build the model. Will use Linear Regression\n",
    "# PyTorch models are class based\n",
    "class LinearRegressionModel(nn.Module): # Basically every neural network model inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__() # also calling init from parent class!\n",
    "\n",
    "        ### Initialize parameters for the model ####\n",
    "        # Parameter is like a helper class that takes in a tensor and adds it to the model appropriately\n",
    "        # For more complicated models, the input tensor will have higher rank\n",
    "        # Want grad to be true so that PyTorch can do autograd for backprop\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "        # Now to create another Parameter for the bias variable\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "\n",
    "    ### Define the forward pass logic ###\n",
    "    # The forward pass definition is the function \"skeleton\" that should be used\n",
    "    # to predict the correct output labels from the input training data\n",
    "    # Of course, we'll use a first order linear function here\n",
    "    # NOTE: forward() is REQUIRED when defining a subclass of nn.Module\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        X is input tensor\n",
    "        \"\"\"\n",
    "        return self.weights * X + self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets inspect the model\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(tuple(model.parameters()))\n",
    "\n",
    "# Can also see a dict of the parameters with their associated values\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how the model predicts values using the initialized parameters\n",
    "# can use inference mode context manager. Context manager turns off autograd\n",
    "# since inference is for predictions only.\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "print(y_preds)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is obviously way off, negative slope and huge shift in interecept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimization functions\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = SGD(params=model.parameters(),\n",
    "                lr=0.1) # lr is learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a training and testing loop to tune the parameters of our model\n",
    "# and view how it's doing periodically\n",
    "\n",
    "# These lists will be used to store key values at different stages of training\n",
    "# to show the model changing\n",
    "epoch_count = []\n",
    "train_loss_vals = []\n",
    "test_loss_vals = []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training Code ###\n",
    "    model.train() # put model in training mode\n",
    "    train_preds = model(train_X) # get models current predictions; NOTE: this will flag autograd without context manager\n",
    "    train_loss = loss_fn(train_preds, train_y) # calculate a tensor with the loss values\n",
    "    optimizer.zero_grad() # zero out the calculated gradients, it's accumulated by default\n",
    "    train_loss.backward() # backprop to calculate gradients\n",
    "    optimizer.step() # update weights based on calculated gradients\n",
    "\n",
    "    ### Testing Code ###\n",
    "    model.eval() # put model in evaluate mode\n",
    "    with torch.inference_mode():\n",
    "        test_preds = model(test_y) # get test preds after most recent param update\n",
    "        test_loss = loss_fn(test_preds, test_y) # get the loss between test preds and test labels\n",
    "    \n",
    "    ### Diag Code ###\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_vals.append(train_loss)\n",
    "        test_loss_vals.append(test_loss)\n",
    "        print(f'Epoch: {epoch} | MAE Training Loss: {train_loss} | MAE Test Loss: {test_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see how the model predictions match the true values\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is much better than before, but still off slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets bundle the loop into a function so that the hyperparameters can be tuned\n",
    "def train_model(feature_train: torch.Tensor,\n",
    "                feature_test: torch.Tensor,\n",
    "                label_train: torch.Tensor,\n",
    "                label_test: torch.Tensor,\n",
    "                model: LinearRegressionModel,\n",
    "                epochs: int = 100,\n",
    "                lr: int = 0.1) -> dict:\n",
    "\n",
    "    # Define loss and optimization functions\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = SGD(params=model.parameters(),\n",
    "                    lr=lr) # lr is learning rate\n",
    "\n",
    "    # Define lists for useful values\n",
    "    epoch_count = []\n",
    "    train_loss_vals = []\n",
    "    test_loss_vals = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ### Training Code ###\n",
    "        model.train() # put model in training mode\n",
    "        train_preds = model(feature_train) # get models current predictions; NOTE: this will flag autograd without context manager\n",
    "        train_loss = loss_fn(train_preds, label_train) # calculate a tensor with the loss values\n",
    "        optimizer.zero_grad() # zero out the calculated gradients, it's accumulated by default\n",
    "        train_loss.backward() # backprop to calculate gradients\n",
    "        optimizer.step() # update weights based on calculated gradients\n",
    "\n",
    "        ### Testing Code ###\n",
    "        model.eval() # put model in evaluate mode\n",
    "        with torch.inference_mode():\n",
    "            test_preds = model(label_test) # get test preds after most recent param update\n",
    "            test_loss = loss_fn(test_preds, label_test) # get the loss between test preds and test labels\n",
    "\n",
    "        ### Diag Code ###\n",
    "        if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_vals.append(train_loss)\n",
    "            test_loss_vals.append(test_loss)\n",
    "            print(f'Epoch: {epoch} | MAE Training Loss: {train_loss} | MAE Test Loss: {test_loss}')\n",
    "    \n",
    "    # Return useful functions as dict\n",
    "    return {\n",
    "        'epochs': epoch_count,\n",
    "        'train_loss': train_loss_vals,\n",
    "        'test_loss': test_loss_vals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the epoch count (while also reinitializing the model to get original weights)\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(model.state_dict())\n",
    "\n",
    "_ = train_model(\n",
    "    train_X,\n",
    "    test_X,\n",
    "    train_y,\n",
    "    test_y,\n",
    "    model,\n",
    "    200,\n",
    "    0.1\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is much better than the naive one, but still off slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the epoch count and lr (while also reinitializing the model to get original weights)\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(model.state_dict())\n",
    "\n",
    "_ = train_model(\n",
    "    train_X,\n",
    "    test_X,\n",
    "    train_y,\n",
    "    test_y,\n",
    "    model,\n",
    "    200,\n",
    "    0.01\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # Changing the learning rate slowed down learning a lot, lets up the epoch count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing the epoch count (while also reinitializing the model to get original weights)\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(model.state_dict())\n",
    "\n",
    "_ = train_model(\n",
    "    train_X,\n",
    "    test_X,\n",
    "    train_y,\n",
    "    test_y,\n",
    "    model,\n",
    "    1000,\n",
    "    0.04\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is much better than before, but still off slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iq_ml_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bbff50bfc5161648b6e194634e05a5fcf9698595f74ac8274cfa4850e6a92e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
