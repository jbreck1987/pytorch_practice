{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # contains all the neural network building blocks\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this notebook is to explore an example PyTorch workflow\n",
    "1. Data Preparation\n",
    "2. Building or using an existing model\n",
    "3. Fitting the model to the data (training)\n",
    "4. Making precitions and evaluating a model (inference)\n",
    "5. Saving and loading a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data (prep and load)\n",
    "The original data can come in many forms. The goal is to reformat the data numerically such that it can be represented by tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets synthesize the input data by using the output of a 1st degree polynomial with known slope (weight) and intercept (bias)\n",
    "def lin_reg(X: torch.Tensor, W: torch.Tensor, b: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Returns the result of evaluating a first order polynomial given the weight and bias values and an input\n",
    "    vector\n",
    "\n",
    "    Inputs\n",
    "        x: independent variable vector (input)\n",
    "        W: the weight value (slope)\n",
    "        b: the bias value (intercept)\n",
    "\n",
    "    Outputs\n",
    "        tensor of size x.shape, W.shape\n",
    "    \"\"\"\n",
    "\n",
    "    return W * X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of this cell will be used as input to train (and test, using different input range) the model\n",
    "\n",
    "# Define range\n",
    "start = 0\n",
    "stop = 1\n",
    "step = 0.02\n",
    "\n",
    "# Define bias and weight values\n",
    "weight = 0.7 # slope\n",
    "bias = 0.3 # intercept\n",
    "\n",
    "X = torch.arange(start, stop, step).unsqueeze(dim=1) # Creating a column vector\n",
    "y = lin_reg(X, weight, bias)\n",
    "print(X[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to create a model that can learn the mathematical function that takes the feature variables (train_X) to the labels (train_y)\n",
    "# First we, need to divide the data (features AND labels) into 3 distinct sets: Test, Validation, Training\n",
    "# For this example, only train and test sets will be used\n",
    "\n",
    "# Train set is used to train the model (typically 80% of the data)\n",
    "train_stop = int(0.8 * len(X))\n",
    "train_X = X[:train_stop]\n",
    "train_y = y[:train_stop]\n",
    "\n",
    "# Test set is used to test accuracy of model on unseen data (20% of the data in this case)\n",
    "test_X = X[train_stop:]\n",
    "test_y = y[train_stop:]\n",
    "\n",
    "print(f'Test X Len: {len(test_X)}\\nTrain X: {len(train_X)}\\nTest y: {len(test_y)}\\nTrain y: {len(train_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to create a function to visualize the data along with the models predictions (no predictions yet...)\n",
    "def plot_data(feature_train: torch.Tensor, feature_test: torch.Tensor, label_train: torch.Tensor, label_test: torch.Tensor, preds: torch.Tensor=None):\n",
    "    \"\"\"\n",
    "    Creates plot depicting the output vs feature, output in {training, test, and prediction data}\n",
    "\n",
    "    Inputs\n",
    "        feature_train: Tensor with the feature training data\n",
    "        feature_test: Tensor with the feature test data\n",
    "        output_train: Tensor with the output training data\n",
    "        output_test: Tensor with the output test data\n",
    "        preds: Tensor with the predictions of model given the feature_test data as input\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Labels')\n",
    "    # plot the feature vs output for training data\n",
    "    plt.scatter(feature_train, label_train, c='b', s=3, label='Training Data')\n",
    "    # plot the feature vs output for test data\n",
    "    plt.scatter(feature_test, label_test, c='g', s=3, label='Test Data')\n",
    "    # plot the feature vs prediction (feature_test as input)\n",
    "    if preds is not None:\n",
    "        plt.scatter(feature_test, preds, c='r', s=3, label='Predictions')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As expected the function between the feature data and the output data is linear, based on the defined function\n",
    "plot_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to build the model. Will use Linear Regression\n",
    "# PyTorch models are class based\n",
    "class LinearRegressionModel(nn.Module): # Basically every neural network model inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__() # also calling init from parent class; req'd because we will be overloading methods from parent\n",
    "\n",
    "        ### Initialize parameters for the model ####\n",
    "\n",
    "        # Parameter is like a helper class that takes in a tensor and adds it to the model appropriately.\n",
    "        # For more complicated models, the input tensor can have higher rank or instance variable will use other classes, like \"nn.Conv1d\"\n",
    "        # Want grad to be true so that PyTorch can do autograd for backprop\n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "        # Now to create another Parameter for the bias variable\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "\n",
    "    ### Define the forward pass logic ###\n",
    "\n",
    "    # The forward pass function defines the computation that should be used\n",
    "    # to predict the correct output labels from the input training data.\n",
    "    # Of course, we'll use a first order linear function here:\n",
    "    # NOTE: forward() is REQUIRED when defining a subclass of nn.Module (it overrides parent class instance method forward())\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        X is input tensor\n",
    "        \"\"\"\n",
    "        return self.weights * X + self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets inspect the model to see the parameters we've created and their values\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(tuple(model.parameters())) # method returns a generator\n",
    "\n",
    "# Can also see a dict of the parameters with their associated values\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how the model predicts values using the initialized parameters\n",
    "# can use inference mode context manager. Context manager turns off autograd\n",
    "# since inference is for predictions only.\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "print(y_preds)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is obviously way off, negative slope and huge shift in interecept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimization functions\n",
    "loss_fn = nn.L1Loss() # This is the mean squared error (can also use summed square error)\n",
    "optimizer = SGD(params=model.parameters(), # stochastic graident descent\n",
    "                lr=0.1) # lr is learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a training and testing loop to tune the parameters of our model\n",
    "# and view how it's doing periodically.\n",
    "\n",
    "# These lists will be used to store key values at different stages of training\n",
    "# to show the model changing\n",
    "epoch_count = []\n",
    "train_loss_vals = []\n",
    "test_loss_vals = []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training Code ###\n",
    "    model.train() # put model in training mode\n",
    "    train_preds = model(train_X) # get models current predictions; NOTE: this will flag autograd without context manager\n",
    "    train_loss = loss_fn(train_preds, train_y) # calculate a tensor with the loss values\n",
    "    optimizer.zero_grad() # zero out the calculated gradients, it's accumulated by default\n",
    "    train_loss.backward() # backprop to calculate gradients\n",
    "    optimizer.step() # update weights based on calculated gradients\n",
    "\n",
    "    ### Testing Code ###\n",
    "    model.eval() # put model in evaluate mode\n",
    "    with torch.inference_mode():\n",
    "        test_preds = model(test_y) # get test preds after most recent param update\n",
    "        test_loss = loss_fn(test_preds, test_y) # get the loss between test preds and test labels\n",
    "    \n",
    "    ### Diag Code ###\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_vals.append(train_loss)\n",
    "        test_loss_vals.append(test_loss)\n",
    "        print(f'Epoch: {epoch} | MAE Training Loss: {train_loss} | MAE Test Loss: {test_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see how the model predictions match the true values after initial training\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is much better than before, but still off slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets bundle the loop into a function so that the hyperparameters can be tuned faster\n",
    "def train_model(model: LinearRegressionModel,\n",
    "                feature_train: torch.Tensor = train_X,\n",
    "                feature_test: torch.Tensor = test_X,\n",
    "                label_train: torch.Tensor = train_y,\n",
    "                label_test: torch.Tensor = test_y,\n",
    "                epochs: int = 100,\n",
    "                lr: float = 0.1) -> dict:\n",
    "\n",
    "    # Define loss and optimization functions\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = SGD(params=model.parameters(),\n",
    "                    lr=lr) # lr is learning rate\n",
    "\n",
    "    # Define lists for useful values\n",
    "    epoch_count = []\n",
    "    train_loss_vals = []\n",
    "    test_loss_vals = []\n",
    "\n",
    "    # Train/Test loop\n",
    "    for epoch in range(epochs):\n",
    "        ### Training Code ###\n",
    "        model.train() # put model in training mode\n",
    "        train_preds = model(feature_train) # get models current predictions; NOTE: this will flag autograd without context manager\n",
    "        train_loss = loss_fn(train_preds, label_train) # calculate a tensor with the loss values\n",
    "        optimizer.zero_grad() # zero out the calculated gradients, it's accumulated by default\n",
    "        train_loss.backward() # backprop to calculate gradients\n",
    "        optimizer.step() # update weights based on calculated gradients\n",
    "\n",
    "        ### Testing Code ###\n",
    "        model.eval() # put model in evaluate mode\n",
    "        with torch.inference_mode():\n",
    "            test_preds = model(label_test) # get test preds after most recent param update\n",
    "            test_loss = loss_fn(test_preds, label_test) # get the loss between test preds and test labels\n",
    "\n",
    "        ### Diag Code ###\n",
    "        if epoch % 10 == 0: # record progress every 10 epochs\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_vals.append(train_loss.item())\n",
    "            test_loss_vals.append(test_loss.item())\n",
    "            print(f'Epoch: {epoch} | MAE Training Loss: {train_loss} | MAE Test Loss: {test_loss}')\n",
    "    \n",
    "    # Return useful values as dict\n",
    "    return {\n",
    "        'epochs': epoch_count,\n",
    "        'train_loss': train_loss_vals,\n",
    "        'test_loss': test_loss_vals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also define a function that plots the training and test losses as a function of epoch\n",
    "def plot_loss(train_loss_vals: list, test_loss_vals: list, epoch_vals: list) -> None:\n",
    "    \"\"\"\n",
    "    Creates plot depicting the loss curves as function of epoch\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    # plot the test loss vs epoch\n",
    "    plt.plot(epoch_vals, test_loss_vals, c='b', label='Test Loss')\n",
    "    # plot the train loss vs epoch\n",
    "    plt.plot(epoch_vals, train_loss_vals, c='g', label='Training Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the epoch count (while also reinitializing the model to get original weights)\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(model.state_dict())\n",
    "\n",
    "vals = train_model(\n",
    "    model,\n",
    "    epochs=200,\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_loss(vals['train_loss'], vals['test_loss'], vals['epochs'])\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is much better than the naive one, but still off slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the epoch count and lr (while also reinitializing the model to get original weights)\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(model.state_dict())\n",
    "\n",
    "vals = train_model(\n",
    "    model,\n",
    "    epochs=200,\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_loss(vals['train_loss'], vals['test_loss'], vals['epochs'])\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # Changing the learning rate slowed down learning a lot, lets up the epoch count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing the epoch count (while also reinitializing the model to get original weights)\n",
    "seed = torch.manual_seed(6.28)\n",
    "model = LinearRegressionModel()\n",
    "print(model.state_dict())\n",
    "\n",
    "vals = train_model(\n",
    "    model,\n",
    "    epochs=1000,\n",
    "    lr=0.04\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X)\n",
    "plot_loss(vals['train_loss'], vals['test_loss'], vals['epochs'])\n",
    "plot_data(train_X, test_X, train_y, test_y, y_preds) # The prediction is much better than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try moving everything to the GPU and doing inference there (NOTE: Seems like training on the M1 isn't supported yet (autograd not supp))\n",
    "model.to('mps')\n",
    "test_X = test_X.to('mps')\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds = model(test_X) # inference successfully done on the GPU!\n",
    "plot_data(train_X, test_X.to(device='cpu'), train_y, test_y, y_preds.to(device='cpu')) # Have to move the tensors back to CPU before using NumPy (called by plt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a trained model\n",
    "\n",
    "Saving a trained model to use for inference later is important for portability. The most common way is by saving the trained model's `state_dict`, which contains the optimized model parameters for a given training set. Pytorch doesn't have a defined data structure for this, it uses Pickle to just serialize the `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directory\n",
    "MODEL_DIR = '../../data/models/'\n",
    "MODEL_FN = 'lin_reg_model_0.pth'\n",
    "\n",
    "# Define function to save the model\n",
    "def save_model(model_dir: str, filename: str,  model: nn.Module) -> None:\n",
    "    print(f'Saving model state_dict to {model_dir + filename}')\n",
    "    torch.save(obj=model.state_dict(), f=model_dir + filename)\n",
    "\n",
    "save_model(MODEL_DIR, MODEL_FN, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the trained model parameters are saved, can create a new model instance\n",
    "# and load the saved parameters.\n",
    "\n",
    "loaded_model = LinearRegressionModel()\n",
    "loaded_model.load_state_dict(torch.load(MODEL_DIR + MODEL_FN)) # self_documenting code here!\n",
    "print(tuple(loaded_model.parameters())) \n",
    "print(tuple(model.parameters())) # note how the two parameters are identical!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iq_ml_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bbff50bfc5161648b6e194634e05a5fcf9698595f74ac8274cfa4850e6a92e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
